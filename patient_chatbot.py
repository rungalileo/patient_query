"""
Patient Query Q&A Chatbot using Chainlit and Langchain.
This application allows patients to ask medical questions and get personalized advice
based on their medical history using OpenAI embeddings and in-memory similarity search.
"""

import os
import time
from typing import cast, List, Dict, Any
from dotenv import load_dotenv, find_dotenv
from colorama import init, Fore, Back, Style

import chainlit as cl
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
from langchain.schema.runnable import Runnable
from langchain.schema.runnable.config import RunnableConfig
from patient_data_processor import patient_processor
from galileo import GalileoLogger, GalileoScorers
from galileo_protect import ainvoke, Rule, RuleOperator, Ruleset, Payload, OverrideAction

# Initialize colorama for cross-platform colored output
init()

# Load environment variables
# 1) load global/shared first
load_dotenv(os.path.expanduser("~/.config/secrets/myapps.env"), override=False)
# 2) then load per-app .env (if present) to override selectively
load_dotenv(find_dotenv(usecwd=True), override=True)

# Initialize Galileo logger once at module level
galileo_logger = None
galileo_project = None
galileo_log_stream = None
protect_enabled = False

# Initialize Galileo logger if configuration is available
api_key = os.getenv("GALILEO_API_KEY")
project = os.getenv("GALILEO_PROJECT")
log_stream = os.getenv("GALILEO_LOG_STREAM")

print(f"Galileo Configuration:")
print(f"  API Key: {'Set' if api_key else 'Not set'}")
print(f"  Project: {project}")
print(f"  Log Stream: {log_stream}")

if all([api_key, project, log_stream]):
    galileo_project = project
    galileo_log_stream = log_stream
    galileo_logger = GalileoLogger(project=project, log_stream=log_stream)
    print(Fore.GREEN + "Galileo logger initialized successfully." + Style.RESET_ALL)
    
    # Initialize Galileo Protect rulesets (using non-Luna metrics for free tier)
    try:
        # Create input protection rulesets
        input_rulesets = [
            Ruleset(
                rules=[Rule(metric=GalileoScorers.prompt_injection, operator=RuleOperator.any, target_value=["impersonation", "obfuscation"])],
                action=OverrideAction(choices=["I detected an unusual request. Please ask a legitimate medical question."])
            ),
            Ruleset(
                rules=[Rule(metric=GalileoScorers.input_toxicity, operator=RuleOperator.gt, target_value=0.5)],
                action=OverrideAction(choices=["Please rephrase your question in a respectful manner."])
            ),
            Ruleset(
                rules=[Rule(metric=GalileoScorers.input_pii, operator=RuleOperator.any, target_value=["ssn", "credit card", "name", "address", "phone number", "email", "social security number", "credit card number", "bank account number", "routing number", "driver's license number", "passport number", "social security number", "credit card number", "bank account number", "routing number", "driver's license number", "passport number"])],
                action=OverrideAction(choices=["Please avoid sharing sensitive personal information."])
            )
        ]
        
        # Create output protection rulesets
        output_rulesets = [
            Ruleset(
                rules=[Rule(metric=GalileoScorers.output_pii, operator=RuleOperator.any, target_value=["ssn", "credit card", "name", "address", "phone number", "email", "social security number", "credit card number", "bank account number", "routing number", "driver's license number", "passport number", "social security number", "credit card number", "bank account number", "routing number", "driver's license number", "passport number"])],
                action=OverrideAction(choices=["I apologize, but I cannot share that information. Please consult your healthcare provider."])
            ),
            Ruleset(
                rules=[Rule(metric=GalileoScorers.output_toxicity, operator=RuleOperator.gt, target_value=0.5)],
                action=OverrideAction(choices=["I apologize, but I cannot provide that information. Please consult a healthcare professional."])
            )
        ]
        
        # Store rulesets for local stage usage
        globals()['input_rulesets'] = input_rulesets
        globals()['output_rulesets'] = output_rulesets
        
        protect_enabled = True
        print(Fore.GREEN + "Galileo Protect guardrails initialized successfully." + Style.RESET_ALL)
        print(f"  Input Protection: Prompt Injection, Toxicity, PII")
        print(f"  Output Protection: PII, Toxicity")
        
    except Exception as e:
        print(Fore.YELLOW + f"Warning: Could not initialize Galileo Protect: {e}" + Style.RESET_ALL)
        print("Protect guardrails will be disabled.")
        protect_enabled = False
else:
    print("Warning: Missing Galileo configuration. Logging and protection will be disabled.")

def extract_patient_name(query: str) -> str:
    """Extract patient name from query using OpenAI."""
    if not query or len(query.strip()) < 5:  # Skip very short queries
        return None
    
    try:
        # Use OpenAI to extract any name from the sentence
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", """Extract the person's name from the given sentence. 
            
            Look for a person's name mentioned in the text. This could be:
            - The speaker introducing themselves
            - A doctor or nurse asking about a patient
            - Someone being referred to
            - Any name in the sentence
            
            Return ONLY the person's name, nothing else. If no name is found, return "None".
            
            Examples:
            - "I am John Smith and I have a headache" ‚Üí "John Smith"
            - "I'm Sarah, can you help me?" ‚Üí "Sarah"
            - "My name is Michael Chen" ‚Üí "Michael Chen"
            - "I am Atin Sanyal" ‚Üí "Atin Sanyal"
            - "Can you help Emily Rodriguez?" ‚Üí "Emily Rodriguez"
            - "I have a fever" ‚Üí "None"
            - "The patient Sarah Johnson needs medication" ‚Üí "Sarah Johnson"
            - Can I give Atin Sanyal amoxicillin for a sinus infection?
            - What‚Äôs a good painkiller for Sarah Johnson‚Äôs headache?
            - Can I prescribe ibuprofen to Michael Chen for back pain?
            - Which medication works well for Emily Rodriguez‚Äôs depression and anemia?
            """),
            ("human", f"Extract the person's name from this sentence: {query}")
        ])
        
        chain = prompt | llm | StrOutputParser()
        result = chain.invoke({})
        
        # Clean up the result
        name = result.strip()
        if name.lower() in ["none", "no name", "not found", ""]:
            return None
        
        # Remove any extra punctuation or words
        name = name.replace('"', '').replace("'", "").strip()
        
        return name if name else None
        
    except Exception as e:
        print(f"Error extracting patient name with OpenAI: {e}")
        return None

def search_patient_records(patient_name: str) -> List[Dict[str, Any]]:
    """Search for patient records using similarity search."""
    if not patient_name:
        return []
    

    if not patient_processor.all_documents or not patient_processor.all_embeddings:
        print("Warning: Documents not loaded. Returning empty results.")
        return []
    
    results = patient_processor.search_patient_records(patient_name)
    
    return [doc for doc, score in results]

def search_medication_info(medication_name: str) -> List[Dict[str, Any]]:
    """Search for medication information using similarity search."""
    if not medication_name:
        return []
    
    if not patient_processor.all_documents or not patient_processor.all_embeddings:
        print("Warning: Documents not loaded. Returning empty results.")
        return []
    
    results = patient_processor.search_medication_info(medication_name)
    
    return [doc for doc, score in results]

def search_symptom_info(symptom: str) -> List[Dict[str, Any]]:
    """Search for symptom information using similarity search."""
    if not symptom:
        return []
    
    if not patient_processor.all_documents or not patient_processor.all_embeddings:
        print("Warning: Documents not loaded. Returning empty results.")
        return []
    
    results = patient_processor.search_symptom_info(symptom)
    
    return [doc for doc, score in results]

def check_contraindications(patient_info: str, medication_info: str, glog=None) -> str:
    """Check for contraindications between patient and medication."""
    if not patient_info or not medication_info:
        return ""
    
    start_time = time.time()
    
    # Use OpenAI to analyze contraindications
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """You are a medical assistant analyzing potential contraindications between a patient's medical history and a medication. 
        
        Analyze the patient information and medication information provided. Look for:
        1. Allergies to the medication or similar drugs
        2. Medical conditions that contraindicate the medication
        3. Drug interactions with current medications
        4. Age-related contraindications
        
        If you find any contraindications, explain them clearly and recommend against taking the medication.
        If no contraindications are found, state that the medication appears safe based on the provided information.
        
        Always recommend consulting a healthcare provider for final medical decisions."""),
        ("human", f"""
        Patient Information:
        {patient_info}
        
        Medication Information:
        {medication_info}
        
        Please analyze for contraindications and provide a clear recommendation.
        """)
    ])
    
    chain = prompt | llm | StrOutputParser()
    
    try:
        result = chain.invoke({})
        
        # Log the contraindication analysis to Galileo (if available)
        if glog:
            try:
                print(f"[Galileo] Adding contraindication analysis span to trace...")
                glog.add_llm_span(
                    input=f"Patient Info: {patient_info}\nMedication Info: {medication_info}",
                    output=result,
                    name="Contraindication Analysis",
                    model="gpt-4o-mini",
                    duration_ns=int((time.time() - start_time) * 1000000),
                    metadata={
                        "source": "patient_chatbot.py",
                        "type": "contraindication_analysis"
                    }
                )
                print(f"[Galileo] Contraindication analysis span added successfully to project '{galileo_project}', log stream '{galileo_log_stream}'.")
            except Exception as e:
                print(f"Warning: Could not log to Galileo (project: {galileo_project}, log stream: {galileo_log_stream}): {e}")
        
        return result
    except Exception as e:
        error_msg = f"Error analyzing contraindications: {str(e)}"
        
        # Log the error to Galileo (if available)
        if glog:
            try:
                print(f"[Galileo] Adding contraindication analysis error span to trace...")
                glog.add_llm_span(
                    input=f"Patient Info: {patient_info}\nMedication Info: {medication_info}",
                    output=error_msg,
                    name="Contraindication Analysis Error",
                    model="gpt-4o-mini",
                    duration_ns=int((time.time() - start_time) * 1000000),
                    metadata={
                        "source": "patient_chatbot.py",
                        "type": "contraindication_analysis_error",
                        "error": str(e)
                    }
                )
                print(f"[Galileo] Contraindication analysis error span added successfully to project '{galileo_project}', log stream '{galileo_log_stream}'.")
            except Exception as log_error:
                print(f"Warning: Could not log error to Galileo (project: {galileo_project}, log stream: {galileo_log_stream}): {log_error}")
        
        return error_msg

@cl.on_chat_start
async def on_chat_start():
    """Initialize the chat session."""
    await cl.Message(
        content="üîÑ Loading patient data and creating embeddings... This may take a moment."
    ).send()
    
    try:
        # Load all documents and create embeddings
        patient_processor.load_all_documents()
        await cl.Message(
            content="‚úÖ Patient data loaded successfully! I'm ready to help you with your health questions."
        ).send()
    except Exception as e:
        await cl.Message(
            content=f"‚ùå Error loading patient data: {str(e)}"
        ).send()
        return

@cl.on_message
async def on_message(message: cl.Message):
    """Handle incoming messages from the user."""
    user_query = message.content
    start_time = time.time()

    try:
        if galileo_logger:
            print(f"Starting Galileo trace for query: {user_query[:50]}...")
            galileo_logger.start_trace(
                input=user_query,
                name=f"user_query: {user_query[:50]}...",
                tags=["patient"]
            )
            print(f"Galileo trace started successfully")

        # Apply input protection guardrails
        if protect_enabled:
            try:
                print(Fore.CYAN + "üõ°Ô∏è  Checking input..." + Style.RESET_ALL)
                payload = Payload(input=user_query)
                protect_response = await ainvoke(
                    payload=payload,
                    prioritized_rulesets=input_rulesets,
                    project_name=galileo_project,
                    stage_name="local_input_protection"
                )
                
                # Debug: print the full response
                print(f"DEBUG: Protect response status: {protect_response.status if protect_response else 'None'}")
                print(f"DEBUG: Protect response text: {protect_response.text if protect_response else 'None'}")
                if protect_response:
                    print(f"DEBUG: Protect response trace_metadata: {protect_response.trace_metadata}")
                
                if protect_response and protect_response.status == "triggered":
                    # Use the text from the response (from OverrideAction choices)
                    safe_msg = protect_response.text or "Please rephrase your question."
                    
                    print(Fore.YELLOW + f"‚ö†Ô∏è  Input blocked: {safe_msg}" + Style.RESET_ALL)
                    await cl.Message(content=f"üõ°Ô∏è {safe_msg}").send()
                    
                    if galileo_logger:
                        galileo_logger.conclude(output=safe_msg, duration_ns=int((time.time() - start_time) * 1000000), status_code=200)
                        galileo_logger.flush()
                    return
                    
                print(Fore.GREEN + f"‚úÖ Input passed (status: {protect_response.status if protect_response else 'None'})" + Style.RESET_ALL)
            except Exception as e:
                print(Fore.YELLOW + f"Warning: Input protection check failed: {e}" + Style.RESET_ALL)

        patient_name = extract_patient_name(user_query)
        print(Fore.MAGENTA + f"Patient name extracted from query: {patient_name}" + Style.RESET_ALL)
        patient_records = search_patient_records(patient_name) if patient_name else []
        print(Fore.YELLOW + f"Patient records: {patient_records}" + Style.RESET_ALL)
        if galileo_logger:
            galileo_logger.add_retriever_span(
                input=patient_name or "Patient",
                output=patient_records,
                name="Patient Records Retrieval",
                duration_ns=int((time.time() - start_time) * 1000000),
                metadata={
                    "document_count": str(len(patient_records)),
                    "patient_name": patient_name or "UNK",
                }
            )
            print(Fore.GREEN + "added retriever span." + Style.RESET_ALL)

        # Search for medication information in the query
        medication_keywords = ["medication", "medicine", "drug", "pill", "prescription"]
        medication_info = []
        for keyword in medication_keywords:
            if keyword in user_query.lower():
                words = user_query.split()
                for word in words:
                    if len(word) > 3 and word.isalpha():
                        med_results = search_medication_info(word)
                        medication_info.extend(med_results)
                break
        
        symptom_keywords = ["symptom", "pain", "ache", "fever", "headache", "nausea", "dizziness"]
        symptom_info = []
        for keyword in symptom_keywords:
            if keyword in user_query.lower():
                symptom_results = search_symptom_info(keyword)
                symptom_info.extend(symptom_results)
                break
        
        context_parts = []
        
        if patient_records:
            patient_context = "\n\n".join([doc["content"] for doc in patient_records])
            context_parts.append(f"Patient Information:\n{patient_context}")
        
        if medication_info:
            med_context = "\n\n".join([doc["content"] for doc in medication_info])
            context_parts.append(f"Medication Information:\n{med_context}")
        
        if symptom_info:
            symptom_context = "\n\n".join([doc["content"] for doc in symptom_info])
            context_parts.append(f"Symptom Information:\n{symptom_context}")
        
        context = "\n\n".join(context_parts)
        
        # Create the LLM chain
        llm = ChatOpenAI(model="gpt-4o", temperature=0.7)
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a helpful medical assistant. You have access to patient records, medication information, and symptom data. 
            
            When responding:
            1. Be empathetic and professional
            2. Use the provided context to give personalized advice
            3. Always recommend consulting a healthcare provider for serious medical decisions
            4. If you don't have enough information, ask for clarification
            5. Never make definitive diagnoses - only provide general guidance
            
            If you have patient information, use it to personalize your response.
            If you have medication information, explain it clearly.
            If you have symptom information, provide relevant guidance."""),
            ("human", "{input}")
        ])
        
        chain = prompt | llm | StrOutputParser()
        
        if context:
            full_input = f"Context:\n{context}\n\nUser Question: {user_query}"
        else:
            full_input = user_query
        
        llm_start_time = time.time()
        response = chain.invoke({"input": full_input})
        llm_end_time = time.time()
        
        if galileo_logger:
            print(f"Adding LLM span to Galileo trace...")
            galileo_logger.add_llm_span(
                input=full_input,
                output=response,
                name="Health Bot Response",
                model="gpt-4o",
                duration_ns=int((llm_end_time - llm_start_time) * 1000000),
                metadata={
                    "type": "health_bot_response",
                    "patient_name": patient_name or "",
                    "has_patient_records": str(bool(patient_records)),
                    "has_medication_info": str(bool(medication_info))
                }
            )
            print(Fore.GREEN + "LLM span added successfully." + Style.RESET_ALL)

        contraindication_analysis = ""
        if patient_records and medication_info:
            patient_summary = "\n".join([doc["content"] for doc in patient_records[:2]])  # Use first 2 records
            medication_summary = "\n".join([doc["content"] for doc in medication_info[:2]])  # Use first 2 records
            
            contraindication_analysis = check_contraindications(
                patient_summary, 
                medication_summary, 
                galileo_logger
            )
            
            if contraindication_analysis:
                response += f"\n\n**Important Safety Check:**\n{contraindication_analysis}"
        
        # Apply output protection guardrails
        final_response = response
        if protect_enabled:
            try:
                print(Fore.CYAN + "üõ°Ô∏è  Checking output..." + Style.RESET_ALL)
                output_payload = Payload(input=user_query, output=response)
                protect_resp = await ainvoke(
                    payload=output_payload,
                    prioritized_rulesets=output_rulesets,
                    project_name=galileo_project,
                    stage_name="local_output_protection"
                )
                
                if protect_resp and protect_resp.status == "triggered":
                    # Use the text from the response (from OverrideAction choices)
                    final_response = protect_resp.text or "I apologize, please consult a healthcare professional."
                    print(Fore.YELLOW + f"‚ö†Ô∏è  Output modified for safety" + Style.RESET_ALL)
                else:
                    print(Fore.GREEN + "‚úÖ Output passed" + Style.RESET_ALL)
            except Exception as e:
                print(Fore.YELLOW + f"Warning: Output protection check failed: {e}" + Style.RESET_ALL)
        
        await cl.Message(content=final_response).send()
        
        if galileo_logger:
            print(f"Concluding Galileo trace...")
            galileo_logger.conclude(
                output=final_response,
                duration_ns=int((time.time() - start_time) * 1000000),
                status_code=200
            )
            print(f"Flushing Galileo trace to UI...")
            galileo_logger.flush()
            print(Fore.GREEN + "Galileo trace concluded and flushed successfully." + Style.RESET_ALL)
    except Exception as e:
        error_msg = f"I apologize, but I encountered an error while processing your request: {str(e)}"
        
        # Log the error to Galileo (if available)
        if galileo_logger:
            print(Fore.RED + f"Concluding Galileo trace with error. project '{galileo_project}', log stream '{galileo_log_stream}'..." + Style.RESET_ALL)
            galileo_logger.conclude(
                output={"error": str(e)},
                duration_ns=int((time.time() - start_time) * 1000000),
                status_code=500
            )
            print(f"Flushing error trace to Galileo UI...")
            galileo_logger.flush()
            print(Fore.RED + "Error trace flushed successfully." + Style.RESET_ALL)

        await cl.Message(content=error_msg).send()

if __name__ == "__main__":
    # Check required environment variables
    required_vars = ["OPENAI_API_KEY"]
    missing_vars = [var for var in required_vars if not os.getenv(var)]
    
    if missing_vars:
        print(f"Missing required environment variables: {', '.join(missing_vars)}")
        print("Please set them in your .env file")
        exit(1)
    
    print("Patient Query Chatbot is ready!")
    print("Run with: chainlit run patient_chatbot.py") 